<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Person Re-identification in the 3D Space + 第一次跑代码 | Fei's Blog</title><meta name="keywords" content="论文阅读"><meta name="author" content="Yang Zhifei"><meta name="copyright" content="Yang Zhifei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文展示   	      	      论文代码论文阅读背景 人的再识别通常被认为是在不重叠的摄像机中定位人的图像检索问题  In this paper, we argue that the key to learning an effective and scalable person representation is to consider both the human appearanc">
<meta property="og:type" content="article">
<meta property="og:title" content="Person Re-identification in the 3D Space + 第一次跑代码">
<meta property="og:url" content="https://yang-zhifei.github.io/2020/09/04/post4/index.html">
<meta property="og:site_name" content="Fei&#39;s Blog">
<meta property="og:description" content="论文展示   	      	      论文代码论文阅读背景 人的再识别通常被认为是在不重叠的摄像机中定位人的图像检索问题  In this paper, we argue that the key to learning an effective and scalable person representation is to consider both the human appearanc">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xingjiahui/CDN/index_01.png">
<meta property="article:published_time" content="2020-09-04T01:41:55.000Z">
<meta property="article:modified_time" content="2021-08-22T15:25:29.143Z">
<meta property="article:author" content="Yang Zhifei">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/xingjiahui/CDN/index_01.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yang-zhifei.github.io/2020/09/04/post4/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Person Re-identification in the 3D Space + 第一次跑代码',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-08-22 23:25:29'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/70204ad2acf0ef3948732ebebed1fc3c552c7b3bb92cd4e786783587c158df8ff584eef138f7c08ab6a8b6112a4b68ac?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=775085618&amp;fname=favicon2.png&amp;size=750" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/xingjiahui/CDN/index_01.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Fei's Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Person Re-identification in the 3D Space + 第一次跑代码</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-09-04T01:41:55.000Z" title="发表于 2020-09-04 09:41:55">2020-09-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-08-22T15:25:29.143Z" title="更新于 2021-08-22 23:25:29">2021-08-22</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Person Re-identification in the 3D Space + 第一次跑代码"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="论文展示"><a href="#论文展示" class="headerlink" title="论文展示"></a>论文展示</h1><p><br> 

	<div class="row">
    <embed src="https://arxiv.org/pdf/2006.04569.pdf" width="100%" height="550" type="application/pdf">
	</div>


</p>
<br>

<h1 id="论文代码"><a href="#论文代码" class="headerlink" title="论文代码"></a><a target="_blank" rel="noopener" href="https://github.com/layumi/person-reid-3d">论文代码</a></h1><h1 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li><p>人的再识别通常被认为是在不重叠的摄像机中定位人的图像检索问题</p>
</li>
<li><p>In this paper, we argue that the key to learning an effective and scalable person representation is to consider both the human appearance and 3D geometry structure. With the prior knowledge of 3D human geometry information, we could learn a depth-aware model, thus making the representation robust to real-world scenarios. As shown in Figure 1, we map the visible surface to the human mesh,<br>and make the person free from the 2D space. Without the need to worry about the part matching from two different viewpoints, the 3D data structure eases the matching difficulty in nature. The model could concentrate on learning the identity-related features, and dealing with the other variants, such as illumination.</p>
<p>在本文中，我们认为学习有效和可扩展的人的表示的关键是同时考虑人的外观和三维几何结构。有了三维人体几何信息的先验知识，我们可以学习一个深度感知模型，从而使表示鲁棒的真实场景。如图1所示，我们将可见表面映射到人体网格，并将人从2D空间中解放出来。三维数据结构在本质上减轻了零件匹配的难度，无需从两种不同的角度进行零件匹配。该模型可以集中学习与身份相关的特征，并处理其他变量，如光照。</p>
</li>
<li><p>To fully take advantage of the 3D structure and 2D appearance, we propose a novel Omni-scale<br>Graph Network for person re-id in the 3D space, called OG-Net. The model is based on graph<br>neural network (GNN) to communicate between the discrete cloud points of arbitrary locations. In<br>particular, following the spirit of the conventional convolutional neural network (CNN), we adopt<br>the KNN-Graph [30] to capture the information from neighbor points.To leverage multi-scale information in 3D data, we propose the Omni-scale module to aggregate the feature from multiple 3D receptive fields. Given the 3D point cloud and the corresponding color information, OG-Net predicts the person identity and outputs the robust human representation for subsequent matching.</p>
<p>为了充分利用3D结构和2D外观的优势，我们提出了一种新颖的Omni-scale Graph网络，用于3D空间中的人员身份识别，称为OG-Net。 <strong>该模型基于图神经网络（GNN）在任意位置的离散云点之间进行通信。</strong> 特别是，按照传统的卷积神经网络（CNN）的精神，我们采用<strong>KNN-Graph [30]来捕获来自邻居点的信息。</strong>为了利用3D数据中的多尺度信息，我们提出了Omni-scale模块 汇总来自多个3D接收字段的要素。 给定3D点云和相应的颜色信息，OG-Net可以预测人的身份并输出健壮的人像表示以进行后续匹配。</p>
</li>
<li><p>那么什么是KNN-Graph？</p>
<ul>
<li>knn即k邻近分类算法，包括距离KNN和概率KNN。</li>
</ul>
</li>
<li><p>相关工作：<strong>点云</strong>分割—-<a target="_blank" rel="noopener" href="https://www.cnblogs.com/li-yao7758258/p/8182846.html">https://www.cnblogs.com/li-yao7758258/p/8182846.html</a></p>
<p>The point cloud is a flexible geometric representation of 3D data<br>structure.One of the earliest works, i.e., PointNet [15], proposes to<br>leverage the multi-layer perceptron (MLP) networks and max-pooling layer to fuse the information from multiple points.</p>
<p>点云是3D数据结构的灵活几何表示。最早的作品之一，即PointNet [15]，提出利用多层感知器（MLP）网络和最大池化层来融合来自多个点的信息。</p>
<ul>
<li>pointnet:介绍可以看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77019339">https://zhuanlan.zhihu.com/p/77019339</a></li>
</ul>
</li>
<li><p>讨论班想到的一个推断问题</p>
</li>
<li><p>视觉对话<a target="_blank" rel="noopener" href="https://visualdialog.org/">https://visualdialog.org/</a></p>
</li>
</ul>
<h2 id="论文主体结构"><a href="#论文主体结构" class="headerlink" title="论文主体结构"></a>论文主体结构</h2><p>OG-net由模块omni-scale Moudle，所以先要明白omni-scale Moudle的结构</p>
<h3 id="Omni-scale-Module"><a href="#Omni-scale-Module" class="headerlink" title="Omni-scale Module"></a>Omni-scale Module</h3><h4 id="理解Dynamic-Graph-Convolution"><a href="#理解Dynamic-Graph-Convolution" class="headerlink" title="理解Dynamic Graph Convolution"></a>理解Dynamic Graph Convolution</h4><p>该文章是最新出的一篇针对点云数据分类、分割的网路，它是受PointNet以及PointNet++的启发所进行的修改，PointNet只是独立处理每个点，同过最大池化后的全局特征来进行网络输出，但忽视了点之间的局部特征。</p>
<p>动态体现在—–KNN图每次都要重新计算</p>
<p>对距离点x_{i}与离它最近的K个点的特征（由公式）进行<strong>一个通道级的对称聚合</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20190321192247145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTM3MzQ4MA==,size_16,color_FFFFFF,t_70"></p>
<p>动态图卷积提取局部还是全局的特征取决于h（x_{i}，x_{j}）</p>
<p><strong>具体细节</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20190321193511229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTM3MzQ4MA==,size_16,color_FFFFFF,t_70"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39373480/article/details/88724518">https://blog.csdn.net/weixin_39373480/article/details/88724518</a></p>
</blockquote>
<h4 id="omni-scale-Moudle输入输出"><a href="#omni-scale-Moudle输入输出" class="headerlink" title="omni-scale Moudle输入输出"></a>omni-scale Moudle输入输出</h4><p>input:RGB/KNN/location</p>
<p>output:new feature/location</p>
<h4 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h4><ul>
<li><p>location—–&gt;fps算法下采样—–&gt;selected location</p>
</li>
<li><p>根据selected location选择selected feature</p>
</li>
</ul>
<h4 id="我们部署三个分组率不同的分支r-8-16-32-，这三个分支不共享权重。"><a href="#我们部署三个分组率不同的分支r-8-16-32-，这三个分支不共享权重。" class="headerlink" title="我们部署三个分组率不同的分支r ={8,16,32}，这三个分支不共享权重。"></a>我们部署三个分组率不同的分支r ={8,16,32}，这三个分支不共享权重。</h4><p>这样，我们可以像传统的二维神经网络[26]一样，捕获具有不同感受野的信息。每个分支由一个分组层、两个线性层、两个批处理归一化(BN)层、一个压榨激励(SE)块[7]和一个组max池化层来聚合本地信息。具体来说，grouping-r层是对每个点的r个最近点进行采样和复制，然后是线性层、批归一化和SE块。在总结三个分支之前，我们引入SE-block[7]作为一个自适应门函数来重新缩放每个分支的权重x0= x×sigmod(h(x))。</p>
<h4 id="SE-block"><a href="#SE-block" class="headerlink" title="SE-block"></a>SE-block</h4><p><img src="https://pic3.zhimg.com/80/v2-c90409a847db3a33fa5b5995fedbeea6_hd.jpg"></p>
<p><strong>SENet的核心思想在于通过网络根据loss去学习特征权重，使得有效的feature map权重大，无效或效果小的feature map权重小的方式训练模型达到更好的结果</strong>。</p>
<p>$$F_{tr} : X \rightarrow U,  X\in R^{W’\times H’\times C’},U\in R^{W \times H\times C}$$</p>
<p>那么这个Ftr的公式就是下面的公式1（卷积操作，vc表示第c个卷积核，xs表示第s个输入）</p>
<p>$$ u_{c}=v_{c}\cdot X=\sum_{s=1}^{C’}v_{c}^{s} \cdot x^{s}$$</p>
<p>Ftr得到的U就是Figure1中的左边第二个三维矩阵，也叫tensor，或者叫C个大小为H*W的feature map。而uc表示U中第c个二维矩阵，下标c表示channel。</p>
<p>接下来就是<strong>Squeeze</strong>操作，公式非常简单，就是一个global average pooling：</p>
<p>$$ z_{c}= F_{sq}(u_{c})=\frac{1}{W \times H}\sum_{i=1}^{W}\sum_{j=1}^{H}u_{c}(i,j) $$</p>
<p>因此公式2就将H<em>W</em>C的输入转换成1<em>1</em>C的输出，对应Figure1中的Fsq操作。<strong>为什么会有这一步呢？这一步的结果相当于表明该层C个feature map的数值分布情况，或者叫全局信息。</strong></p>
<p>再接下来就是Excitation操作，如公式3。直接看最后一个等号，前面squeeze得到的结果是z，这里先用W1乘以z，就是一个全连接层操作，<strong>W1的维度是C/r * C，这个r是一个缩放参数，在文中取的是16，这个参数的目的是为了减少channel个数从而降低计算量</strong>。又因为z的维度是1<em>1</em>C，所以W1z的结果就是1<em>1</em>C/r；然后再经过一个ReLU层，输出的维度不变；然后再和W2相乘，和W2相乘也是一个全连接层的过程，<strong>W2的维度是C*C/r</strong>，因此输出的维度就是1<em>1</em>C；最后再经过sigmoid函数，得到s。</p>
<p>$$ s = F_{ex}(z,W) = \sigma(g(z,W)) = \sigma(W_{2}\delta(W_{1}z))$$</p>
<p>也就是说最后得到的这个s的维度是1<em>1</em>C，C表示channel数目。<strong>这个s其实是本文的核心，它是用来刻画tensor U中C个feature map的权重。而且这个权重是通过前面这些全连接层和非线性层学习得到的，因此可以end-to-end训练。这两个全连接层的作用就是融合各通道的feature map信息，因为前面的squeeze都是在某个channel的feature map里面操作。</strong></p>
<p>在得到s之后，就可以对原来的tensor U操作了，就是进行channel-wise multiplication。uc是一个二维矩阵，sc是一个数，也就是权重，因此相当于把uc矩阵中的每个值都乘以sc。对应Figure1中的Fscale。</p>
<p>$$ \widetilde{X_{c}} = F_{scale}(u_{c},s_{c}) = s_{c} \cdot u_{c}$$</p>
<blockquote>
<p>SENet（Squeeze-and-Excitation Networks）算法笔记<a target="_blank" rel="noopener" href="https://blog.csdn.net/u014380165/article/details/78006626">https://blog.csdn.net/u014380165/article/details/78006626</a></p>
</blockquote>
<h4 id="最后，我们使用“添加”来计算三个分支的总和而不是连接"><a href="#最后，我们使用“添加”来计算三个分支的总和而不是连接" class="headerlink" title="最后，我们使用“添加”来计算三个分支的总和而不是连接"></a>最后，我们使用<strong>“添加”</strong>来计算三个分支的总和而不是连接</h4><p>你可以这么理解，add是描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加，这显然是对最终的图像的分类是有益的。而concatenate是通道数的合并，也就是说描述图像本身的特征增加了，而每一特征下的信息是没有增加。</p>
<h4 id="omni-scale-Moudle总结"><a href="#omni-scale-Moudle总结" class="headerlink" title="omni-scale Moudle总结"></a>omni-scale Moudle总结</h4><p>To summarize, the key of Omni-scale Module is two cross-point functions. The cross-point function<br>indicates the function considers the neighbor points, while the pre-point function only considers<br>the feature of one point itself. One cross-point function is the dynamic graph convolution before<br>downsampling, which could be simply formulated asPh(xi, xj). It mimics the conventional 2D<br>CNN to aggregate the local patterns according to the position. The other is the max group pooling<br>layer in each branch, which could be simply formulated as maxh(xi). It maximizes neighbor features<br>in each group as the new point feature</p>
<p>综上所述，全尺度模块的关键是两个交叉点函数。交叉点函数表示函数考虑相邻点，而前置点函数只考虑一个点本身的特征。一个交叉点函数是降采样前的动态图卷积，可以简单地表示为asPh(xi, xj)。模拟传统的二维CNN，根据位置聚合局部模式。另一个是每个分支中的max组池化层，可以简单表示为maxh(xi)。将每组中的近邻特征最大化作为新的点特征</p>
<ul>
<li><p>这里用两个全连接层先降维再升维的目的和原理是什么呢？</p>
</li>
<li><p>降维是为了更简单的计算权重部分，而且实验也给出r不同取值对结果的影响；而升维是必须的，这样才能和原来的Uc相乘，得到每一层特征的权重。</p>
</li>
</ul>
<h3 id="OGNet-Architecture"><a href="#OGNet-Architecture" class="headerlink" title="OGNet Architecture"></a>OGNet Architecture</h3><p>OG-Net的结构由四个Omniscale模块组成。与传统CNN一样，我们逐步减少选择点的数量。每减少一个点，外观特征的通道数就增加一倍。经过四个全尺度模块，512-dim外观特征得到96个点。与[30]类似，我们使用max pooling和average pooling对点特征进行聚合，并将两个输出连接起来，得到1024-dim feature。通过添加全连通层和批量归一化层，将特征压缩到512维作为行人表示。</p>
<p>在进行推理（实际应用）时，我们去掉最后一个线性分类器进行借口分类任务，提取512-dim特征进行图像匹配。</p>
<h4 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h4><p><a target="_blank" rel="noopener" href="https://scientificrat.com/2018/06/21/%E7%90%86%E8%A7%A3softmax%E5%88%86%E7%B1%BB%E5%99%A8/">https://scientificrat.com/2018/06/21/%E7%90%86%E8%A7%A3softmax%E5%88%86%E7%B1%BB%E5%99%A8/</a>写的</p>
<h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><p>条件概率是指<strong>事件A在事件B发生的条件下</strong>发生的概率。</p>
<p>$$ P(A \mid B)= \frac{P(AB)}{P(B)}$$  </p>
<p>损失函数$$ L_{id} = \mathbf{E}[-log(p(y_{n} \mid s_{n}))]$$</p>
<p>其中$$p(y_{n} \mid s_{n})$$是sn归属于ground-truth类yn的预测可能性。训练目标要求模型能够根据输入点区分不同的身份。</p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Qualitative-Results"><a href="#Qualitative-Results" class="headerlink" title="Qualitative Results"></a>Qualitative Results</h4><h4 id="Quantitative-Results"><a href="#Quantitative-Results" class="headerlink" title="Quantitative Results"></a>Quantitative Results</h4><h3 id="Further-Analysis-and-Discussions"><a href="#Further-Analysis-and-Discussions" class="headerlink" title="Further Analysis and Discussions"></a>Further Analysis and Discussions</h3><h1 id="实际运行代码时遇到的问题"><a href="#实际运行代码时遇到的问题" class="headerlink" title="实际运行代码时遇到的问题"></a>实际运行代码时遇到的问题</h1><p>实验要求pytoch 1.4.0版本</p>
<h2 id="配置环境失败（这可能是每次都会遇到的）"><a href="#配置环境失败（这可能是每次都会遇到的）" class="headerlink" title="配置环境失败（这可能是每次都会遇到的）"></a>配置环境失败（这可能是每次都会遇到的）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<ul>
<li><p>解决方法一：</p>
<p>查出原因是在执行这一语句前需要先创建requirements.txt 文件，所以必须先执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure></li>
<li><p>解决方法二：</p>
<p>直接打开</p>
</li>
</ul>
<h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><p>数据集在Google Drive里。之前，我都是通过浏览器直接下载的，但是经常下载到一半就失败了。查了很多方法，都没有解决，甚至还让别人帮忙保存到百度云……最后，终于找到了成功下载的办法。</p>
<p><strong>下载速度提升10倍以上，大大节省了时间成本</strong></p>
<ol>
<li>安装IDM。下载IDM(<a href="https://link.zhihu.com/?target=http://www.internetdownloadmanager.com/">IDM官网</a>)，默认安装即可。</li>
<li>如果第一步安装完，chrome中没有成功安装IDM插件，可按照以下步骤安装。</li>
</ol>
<p>​       1）进入官网菜单栏<strong>Support</strong>下的<strong>FAQ</strong>中。</p>
<p>​       2）进入 <a href="https://link.zhihu.com/?target=http://www.internetdownloadmanager.com/register/new_faq/bi_main.cgi?mode=1">BROWSER INTEGRATION QUESTIONS</a></p>
<p>​       3）进入 <a href="https://link.zhihu.com/?target=http://www.internetdownloadmanager.com/register/new_faq/bi31.html">I cannot install IDM extension for Chrome manually. It tells that extensions cannot be added from this site. How can I install it?</a></p>
<p>​       4）点击下载安装。</p>
<blockquote>
<p>​        转自<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/96654477">https://zhuanlan.zhihu.com/p/96654477</a></p>
</blockquote>
<h2 id="服务器下载数据集"><a href="#服务器下载数据集" class="headerlink" title="服务器下载数据集"></a>服务器下载数据集</h2><p>因为文件太大，不能进行移动，demo形式是onedrive和Google drive。</p>
<h3 id="onedrive-耗时2小时解决"><a href="#onedrive-耗时2小时解决" class="headerlink" title="onedrive -耗时2小时解决"></a>onedrive -耗时2小时解决</h3><h4 id="step1：首先打开官方分享界面链接，发现并不是真是下载的地址。点开之后，找个文件点击下载，然后取消，在devtool中输入文件名字，可以找到真实链接"><a href="#step1：首先打开官方分享界面链接，发现并不是真是下载的地址。点开之后，找个文件点击下载，然后取消，在devtool中输入文件名字，可以找到真实链接" class="headerlink" title="step1：首先打开官方分享界面链接，发现并不是真是下载的地址。点开之后，找个文件点击下载，然后取消，在devtool中输入文件名字，可以找到真实链接"></a>step1：首先打开官方分享界面链接，发现并不是真是下载的地址。点开之后，找个文件点击下载，然后取消，在devtool中输入文件名字，可以找到真实链接</h4><h4 id="step2：如果直接在服务器里-wget-这个下载链接，100-会403-Forbidden。有经验的老手都知道肯定是没带-Cookie-，但是要怎么带Cookie呢，其实很简单（？？？）"><a href="#step2：如果直接在服务器里-wget-这个下载链接，100-会403-Forbidden。有经验的老手都知道肯定是没带-Cookie-，但是要怎么带Cookie呢，其实很简单（？？？）" class="headerlink" title="step2：如果直接在服务器里 wget 这个下载链接，100%会403: Forbidden。有经验的老手都知道肯定是没带 Cookie ，但是要怎么带Cookie呢，其实很简单（？？？）"></a>step2：如果直接在服务器里 wget 这个下载链接，100%会403: Forbidden。有经验的老手都知道肯定是没带 Cookie ，但是要怎么带Cookie呢，其实很简单（？？？）</h4><ul>
<li><p>首先你需要按下F12，打开开发人员工具，然后转到网络选项，之后点击想要下载的文件进行下载，同时观察开发人员工具窗口，找到带有download.aspx/?…. 的那个链接</p>
</li>
<li><p>之后在那个链接上右键，选择 复制-复制为cURL命令(POSIX)</p>
</li>
<li><p>会在剪切板出现类似下面的命令语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl &#x27;https://gitaccuacnz2-my.sharepoint.com/personal/mail_finderacg_com/_layouts/15/download.aspx?UniqueId=cb44677f%2Dc4af%2D44ad%2D88d4%2Dceb07d9625da&#x27; -H &#x27;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) Gecko/20100101 Firefox/78.0&#x27; -H &#x27;Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#x27; -H &#x27;Accept-Language: zh-HK,zh-TW;q=0.5&#x27; --compressed -H &#x27;Connection: keep-alive&#x27; -H &#x27;Referer: https://gitaccuacnz2-my.sharepoint.com/personal/mail_finderacg_com/_layouts/15/onedrive.aspx?.......&#x27; -H &#x27;Cookie: CCSInfo=NS8yOS8yMDIwIDEwOjE1OjE0IEFNC/.....; FeatureOverrides_enableFeatures=; FeatureOverrides_disableFeatures=&#x27; -H &#x27;Upgrade-Insecure-Requests: 1&#x27; -H &#x27;Sec-Fetch-Dest: iframe&#x27; -H &#x27;Sec-Fetch-Mode: navigate&#x27; -H &#x27;Sec-Fetch-Site: same-origin&#x27;</span><br></pre></td></tr></table></figure></li>
<li><p>之后粘贴到Linux的命令行里，最后在后面补加一句 <code>--output file.extension</code> ,其中 <code>file.extension</code> 是想要保存的文件名。最终执行的命令就是类似这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl &#x27;https://gitaccuacnz2-my.sharepoint.com/personal/mail_finderacg_com/_layouts/15/download.aspx?UniqueId=cb44677f%2Dc4af%2D44ad%2D88d4%2Dceb07d9625da&#x27; -H &#x27;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) Gecko/20100101 Firefox/78.0&#x27; -H &#x27;Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#x27; -H &#x27;Accept-Language: zh-HK,zh-TW;q=0.5&#x27; --compressed -H &#x27;Connection: keep-alive&#x27; -H &#x27;Referer: https://gitaccuacnz2-my.sharepoint.com/personal/mail_finderacg_com/_layouts/15/onedrive.aspx?.......&#x27; -H &#x27;Cookie: CCSInfo=NS8yOS8yMDIwIDEwOjE1OjE0IEFNC/.....; FeatureOverrides_enableFeatures=; FeatureOverrides_disableFeatures=&#x27; -H &#x27;Upgrade-Insecure-Requests: 1&#x27; -H &#x27;Sec-Fetch-Dest: iframe&#x27; -H &#x27;Sec-Fetch-Mode: navigate&#x27; -H &#x27;Sec-Fetch-Site: same-origin&#x27; --output file.extension</span><br></pre></td></tr></table></figure></li>
<li><p>同时这个方法也适合文件夹下载，只不过前缀是 <code>xxxxx-mediap.svc.ms</code> ，其中<code>xxxxx</code>是地区代号，不同的下载链接所指向的地区不同，具体看情况，这里就不再赘述。</p>
</li>
<li><p>如果使用Chrome下载，需要选择 复制cURL(Bash) 然后把里面的换行和\ 都处理掉，要不然会不行，但是我没成功过，按理讲应该是能成功的，算了，Firefox能用就行了，又不是天天下载。</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000021098910">这篇文章也写得不错</a></p>
<h3 id="什么是cookie？我们如何利用cookie高效下载"><a href="#什么是cookie？我们如何利用cookie高效下载" class="headerlink" title="什么是cookie？我们如何利用cookie高效下载"></a>什么是cookie？我们如何利用cookie高效下载</h3><h3 id="google-drive-未解决"><a href="#google-drive-未解决" class="headerlink" title="google drive 未解决"></a>google drive 未解决</h3></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yang Zhifei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yang-zhifei.github.io/2020/09/04/post4/">https://yang-zhifei.github.io/2020/09/04/post4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yang-zhifei.github.io" target="_blank">Fei's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/xingjiahui/CDN/index_01.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/09/04/post-3/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">师兄的讲义</div></div></a></div><div class="next-post pull-right"><a href="/2020/09/02/post3/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计图</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/08/23/post-1/" title="对抗性reid论文学习中知识补充"><img class="cover" src="https://cdn.jsdelivr.net/gh/xingjiahui/CDN/index_01.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-23</div><div class="title">对抗性reid论文学习中知识补充</div></div></a></div><div><a href="/2020/08/23/post-2/" title="虚拟试衣Try- on论文与评价方法改进的Reid"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-23</div><div class="title">虚拟试衣Try- on论文与评价方法改进的Reid</div></div></a></div><div><a href="/2020/10/07/post-7/" title="Weakly Supervised Discriminative Feature Learning with State Information"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-10-07</div><div class="title">Weakly Supervised Discriminative Feature Learning with State Information</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%B1%95%E7%A4%BA"><span class="toc-number">1.</span> <span class="toc-text">论文展示</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">论文代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB"><span class="toc-number">3.</span> <span class="toc-text">论文阅读</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">3.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%BB%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-number">3.2.</span> <span class="toc-text">论文主体结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Omni-scale-Module"><span class="toc-number">3.2.1.</span> <span class="toc-text">Omni-scale Module</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%A7%A3Dynamic-Graph-Convolution"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">理解Dynamic Graph Convolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#omni-scale-Moudle%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">omni-scale Moudle输入输出</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E9%87%87%E6%A0%B7"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">下采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E9%83%A8%E7%BD%B2%E4%B8%89%E4%B8%AA%E5%88%86%E7%BB%84%E7%8E%87%E4%B8%8D%E5%90%8C%E7%9A%84%E5%88%86%E6%94%AFr-8-16-32-%EF%BC%8C%E8%BF%99%E4%B8%89%E4%B8%AA%E5%88%86%E6%94%AF%E4%B8%8D%E5%85%B1%E4%BA%AB%E6%9D%83%E9%87%8D%E3%80%82"><span class="toc-number">3.2.1.4.</span> <span class="toc-text">我们部署三个分组率不同的分支r &#x3D;{8,16,32}，这三个分支不共享权重。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SE-block"><span class="toc-number">3.2.1.5.</span> <span class="toc-text">SE-block</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E2%80%9C%E6%B7%BB%E5%8A%A0%E2%80%9D%E6%9D%A5%E8%AE%A1%E7%AE%97%E4%B8%89%E4%B8%AA%E5%88%86%E6%94%AF%E7%9A%84%E6%80%BB%E5%92%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E8%BF%9E%E6%8E%A5"><span class="toc-number">3.2.1.6.</span> <span class="toc-text">最后，我们使用“添加”来计算三个分支的总和而不是连接</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#omni-scale-Moudle%E6%80%BB%E7%BB%93"><span class="toc-number">3.2.1.7.</span> <span class="toc-text">omni-scale Moudle总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OGNet-Architecture"><span class="toc-number">3.2.2.</span> <span class="toc-text">OGNet Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">线性分类器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-Objective"><span class="toc-number">3.2.3.</span> <span class="toc-text">Training Objective</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">3.2.4.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Qualitative-Results"><span class="toc-number">3.2.4.1.</span> <span class="toc-text">Qualitative Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Quantitative-Results"><span class="toc-number">3.2.4.2.</span> <span class="toc-text">Quantitative Results</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Further-Analysis-and-Discussions"><span class="toc-number">3.2.5.</span> <span class="toc-text">Further Analysis and Discussions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E8%BF%90%E8%A1%8C%E4%BB%A3%E7%A0%81%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">4.</span> <span class="toc-text">实际运行代码时遇到的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%A4%B1%E8%B4%A5%EF%BC%88%E8%BF%99%E5%8F%AF%E8%83%BD%E6%98%AF%E6%AF%8F%E6%AC%A1%E9%83%BD%E4%BC%9A%E9%81%87%E5%88%B0%E7%9A%84%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">配置环境失败（这可能是每次都会遇到的）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.2.</span> <span class="toc-text">下载数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.3.</span> <span class="toc-text">服务器下载数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#onedrive-%E8%80%97%E6%97%B62%E5%B0%8F%E6%97%B6%E8%A7%A3%E5%86%B3"><span class="toc-number">4.3.1.</span> <span class="toc-text">onedrive -耗时2小时解决</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#step1%EF%BC%9A%E9%A6%96%E5%85%88%E6%89%93%E5%BC%80%E5%AE%98%E6%96%B9%E5%88%86%E4%BA%AB%E7%95%8C%E9%9D%A2%E9%93%BE%E6%8E%A5%EF%BC%8C%E5%8F%91%E7%8E%B0%E5%B9%B6%E4%B8%8D%E6%98%AF%E7%9C%9F%E6%98%AF%E4%B8%8B%E8%BD%BD%E7%9A%84%E5%9C%B0%E5%9D%80%E3%80%82%E7%82%B9%E5%BC%80%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%89%BE%E4%B8%AA%E6%96%87%E4%BB%B6%E7%82%B9%E5%87%BB%E4%B8%8B%E8%BD%BD%EF%BC%8C%E7%84%B6%E5%90%8E%E5%8F%96%E6%B6%88%EF%BC%8C%E5%9C%A8devtool%E4%B8%AD%E8%BE%93%E5%85%A5%E6%96%87%E4%BB%B6%E5%90%8D%E5%AD%97%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%89%BE%E5%88%B0%E7%9C%9F%E5%AE%9E%E9%93%BE%E6%8E%A5"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">step1：首先打开官方分享界面链接，发现并不是真是下载的地址。点开之后，找个文件点击下载，然后取消，在devtool中输入文件名字，可以找到真实链接</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#step2%EF%BC%9A%E5%A6%82%E6%9E%9C%E7%9B%B4%E6%8E%A5%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%87%8C-wget-%E8%BF%99%E4%B8%AA%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5%EF%BC%8C100-%E4%BC%9A403-Forbidden%E3%80%82%E6%9C%89%E7%BB%8F%E9%AA%8C%E7%9A%84%E8%80%81%E6%89%8B%E9%83%BD%E7%9F%A5%E9%81%93%E8%82%AF%E5%AE%9A%E6%98%AF%E6%B2%A1%E5%B8%A6-Cookie-%EF%BC%8C%E4%BD%86%E6%98%AF%E8%A6%81%E6%80%8E%E4%B9%88%E5%B8%A6Cookie%E5%91%A2%EF%BC%8C%E5%85%B6%E5%AE%9E%E5%BE%88%E7%AE%80%E5%8D%95%EF%BC%88%EF%BC%9F%EF%BC%9F%EF%BC%9F%EF%BC%89"><span class="toc-number">4.3.1.2.</span> <span class="toc-text">step2：如果直接在服务器里 wget 这个下载链接，100%会403: Forbidden。有经验的老手都知道肯定是没带 Cookie ，但是要怎么带Cookie呢，其实很简单（？？？）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFcookie%EF%BC%9F%E6%88%91%E4%BB%AC%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8cookie%E9%AB%98%E6%95%88%E4%B8%8B%E8%BD%BD"><span class="toc-number">4.3.2.</span> <span class="toc-text">什么是cookie？我们如何利用cookie高效下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#google-drive-%E6%9C%AA%E8%A7%A3%E5%86%B3"><span class="toc-number">4.3.3.</span> <span class="toc-text">google drive 未解决</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://w.wallhaven.cc/full/39/wallhaven-39god9.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Yang Zhifei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my blog !</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>